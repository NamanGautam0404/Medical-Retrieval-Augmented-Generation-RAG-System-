# -*- coding: utf-8 -*-
"""Neo4j Graph(NLP project)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10UWCjPqI3danlEiDUNmNlS7f96LRpzvg
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs

from langchain_core.runnables import (
    RunnableBranch,
    RunnableLambda,
    RunnableParallel,
    RunnablePassthrough,
)
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts.prompt import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import Tuple, List, Optional
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
import os
from langchain_community.graphs import Neo4jGraph
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain_openai import ChatOpenAI
from langchain_experimental.graph_transformers import LLMGraphTransformer
from neo4j import GraphDatabase
from yfiles_jupyter_graphs import GraphWidget
from langchain_community.vectorstores import Neo4jVector
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars
from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough

try:
  import google.colab
  from google.colab import output
  output.enable_custom_widget_manager()
except:
  pass



# Install necessary dependencies (make sure they are installed first)
!pip install -U langchain-community langchain networkx matplotlib neo4j sentence-transformers transformers

# Import necessary libraries
import os
from neo4j import GraphDatabase
import networkx as nx
import matplotlib.pyplot as plt
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import Neo4jVector
from IPython.display import display
from transformers import pipeline  # For Hugging Face Q&A pipeline

# List of medical topics to load from Wikipedia
medical_queries = [
    "Cancer treatment", "Diabetes", "Heart disease", "Hypertension", "Stroke",
    "Alzheimer's disease", "Mental health", "Asthma", "Obesity", "HIV/AIDS",
    "Tuberculosis", "Kidney disease", "Liver disease", "Arthritis", "Parkinson's disease",
    "Cystic fibrosis", "Multiple sclerosis", "Epilepsy", "Chronic pain", "Infectious diseases",
    "Autoimmune diseases", "Blood disorders", "Hematology", "Dermatology", "Pediatric care",
    "Neonatal care", "Geriatrics", "Surgery", "Orthopedics", "Immunology", "Pharmacology",
    "Vaccination", "Gene therapy", "Radiology", "Pathology", "Medical imaging"
]

# Step 1: Load the Wikipedia articles related to the medical topics
# Loop through the medical topics and load documents for each one
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()


text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents[:3])


embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Step 4: Prepare the documents and their embeddings to upload to the Neo4j graph
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,  # Use HuggingFace embeddings
    search_type="hybrid",      # Hybrid search combines traditional text and vector search
    node_label="Document",     # Label for graph nodes (documents)
    text_node_properties=["text"],  # Store text content in each node's 'text' property
    embedding_node_property="embedding"  # Store embeddings in this property
)

# Add documents to the Neo4j graph
vector_index.add_documents(documents)

# Step 5: Create a graph with NetworkX (optional: for visualization or other operations)
G = nx.Graph()

# Add nodes based on documents (each chunk is a node in the graph)
for i, doc in enumerate(documents):
    G.add_node(i, label=doc.page_content[:100], text=doc.page_content)  # Add first 100 chars as label for visualization
    G.add_edge(i, (i+1) % len(documents), weight=1)  # Create edges between consecutive chunks for context

# Example of printing out the graph nodes and edges
print("Graph nodes:")
for node in G.nodes(data=True):
    print(node)

print("\nGraph edges:")
for edge in G.edges(data=True):
    print(edge)

# Step 6: Optional: Visualize the graph (using NetworkX or Neo4j's own visualization tools)
nx.draw(G, with_labels=True, node_size=700, node_color="lightblue", font_size=12)
plt.show()

# Step 7: Show the Neo4j graph from the given Cypher query
default_cypher = "MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50"

def showGraph(cypher: str = default_cypher):
    # Create a Neo4j session to run queries
    driver = GraphDatabase.driver(
        uri=os.environ["NEO4J_URI"],
        auth=(os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
    )
    session = driver.session()

    # Fetch and display the graph
    widget = GraphWidget(graph=session.run(cypher).graph())
    widget.node_label_mapping = 'id'
    display(widget)
    return widget

# Show the Neo4j graph
showGraph()  # This will display the graph from Neo4j based on the cypher query

# Step 7: Query the Neo4j vector store based on a user question (using vector search)
def query_graph(question):
    # Query the Neo4j graph to find the most relevant document(s) for the input question
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Step 8: Use a free question-answering model from Hugging Face (e.g., DistilBERT, BART)
def generate_response_from_graph(question):
    # Retrieve the relevant document(s) from the graph
    relevant_documents = query_graph(question)

    # Join the relevant document contents to form the input for the language model
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Load Hugging Face's question-answering pipeline (use BART or DistilBERT)
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

    # Get the answer to the question from the context
    answer = qa_pipeline(question=question, context=context)

    return answer['answer']

# Step 9: Example question to ask the graph
question = "how to manage poor diet?"

# Step 10: Generate the response using the graph
response = generate_response_from_graph(question)
print("Response:", response)

"""Now it's time to construct a graph based on the retrieved documents. For this purpose, we have implemented an `LLMGraphTransformermodule` that significantly simplifies constructing and storing a knowledge graph in a graph database."""



from google.colab import output
output.enable_custom_widget_manager()

"""Support for third party widgets will remain active for the duration of the session. To disable support:"""

from google.colab import output
output.disable_custom_widget_manager()

def query_graph(question):
    # Query the Neo4j graph to find the most relevant document(s) for the input question
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Step 8: Use a free question-answering model from Hugging Face (e.g., DistilBERT, BART)
def generate_response_from_graph(question):
    # Retrieve the relevant document(s) from the graph
    relevant_documents = query_graph(question)

    # Join the relevant document contents to form the input for the language model
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Load Hugging Face's question-answering pipeline (use BART or DistilBERT)
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

    # Get the answer to the question from the context
    answer = qa_pipeline(question=question, context=context)

    return answer['answer']

# Step 9: Example question to ask the graph
question = "how to manage poor diet?"

# Step 10: Generate the response using the graph
response = generate_response_from_graph(question)
print("Response:", response)

def query_graph(question):
    # Query the Neo4j graph to find the most relevant document(s) for the input question
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Step 8: Use a free question-answering model from Hugging Face (e.g., DistilBERT, BART)
def generate_response_from_graph(question):
    # Retrieve the relevant document(s) from the graph
    relevant_documents = query_graph(question)

    # Join the relevant document contents to form the input for the language model
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Load Hugging Face's question-answering pipeline (use BART or DistilBERT)
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

    # Get the answer to the question from the context
    answer = qa_pipeline(question=question, context=context)

    return answer['answer']

# Step 9: Example question to ask the graph
question = "how to manage poor diet?"
````
# Step 10: Generate the response using the graph
response = generate_response_from_graph(question)
print("Response:", response)

# directly show the graph resulting from the given Cypher query
default_cypher = "MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50"

def showGraph(cypher: str = default_cypher):
    # create a neo4j session to run queries
    driver = GraphDatabase.driver(
        uri = os.environ["NEO4J_URI"],
        auth = (os.environ["NEO4J_USERNAME"],
                os.environ["NEO4J_PASSWORD"]))
    session = driver.session()
    widget = GraphWidget(graph = session.run(cypher).graph())
    widget.node_label_mapping = 'id'
    #display(widget)
    return widget

showGraph()

vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,  # This is the HuggingFace wrapper we created
    search_type="hybrid",      # Hybrid search combines traditional text and vector search
    node_label="Document",     # Label for the graph nodes (documents)
    text_node_properties=["text"],  # The properties to store in each node
    embedding_node_property="embedding"  # Store embeddings in this property
)

# Install the required dependencies (make sure they are installed first)
!pip install -U langchain-community langchain networkx matplotlib neo4j sentence-transformers

# Import necessary libraries
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings  # Use HuggingFaceEmbeddings for integration
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import Neo4jVector
import networkx as nx
import matplotlib.pyplot as plt

# List of medical topics to load from Wikipedia
medical_queries = [
    "Cancer treatment", "Diabetes", "Heart disease", "Hypertension", "Stroke",
    "Alzheimer's disease", "Mental health", "Asthma", "Obesity", "HIV/AIDS",
    "Tuberculosis", "Kidney disease", "Liver disease", "Arthritis", "Parkinson's disease",
    "Cystic fibrosis", "Multiple sclerosis", "Epilepsy", "Chronic pain", "Infectious diseases",
    "Autoimmune diseases", "Blood disorders", "Hematology", "Dermatology", "Pediatric care",
    "Neonatal care", "Geriatrics", "Surgery", "Orthopedics", "Immunology", "Pharmacology",
    "Vaccination", "Gene therapy", "Radiology", "Pathology", "Medical imaging"
]

# Step 1: Load the Wikipedia articles related to the medical topics
# Loop through the medical topics and load documents for each one
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()
raw_documents = WikipediaLoader(query=medical_query).load()

# Step 2: Define chunking strategy (split the document into chunks)
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents[:3])  # Limit to first 3 chunks

# Step 3: Use SentenceTransformer model and wrap it with HuggingFaceEmbeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")  # Wrap the model correctly

# Step 4: Prepare the documents and their embeddings to upload to the Neo4j graph
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,  # Use the HuggingFace wrapper object
    search_type="hybrid",      # Hybrid search combines traditional text and vector search
    node_label="Document",     # Label for the graph nodes (documents)
    text_node_properties=["text"],  # The properties to store in each node
    embedding_node_property="embedding"  # Store embeddings in this property
)

# Add documents to the Neo4j graph
vector_index.add_documents(documents)

# Step 5: Create a graph with NetworkX (optional: for visualization or other operations)
G = nx.Graph()

# Add nodes based on documents (each chunk is a node in the graph)
for i, doc in enumerate(documents):
    G.add_node(i, label=doc.page_content[:100], text=doc.page_content)  # Add first 100 chars as label for visualization
    G.add_edge(i, (i+1) % len(documents), weight=1)  # Create edges between consecutive chunks for context

# Example of printing out the graph nodes and edges
print("Graph nodes:")
for node in G.nodes(data=True):
    print(node)

print("\nGraph edges:")
for edge in G.edges(data=True):
    print(edge)

# Step 6: Optional: Visualize the graph (using NetworkX or Neo4j's own visualization tools)
nx.draw(G, with_labels=True, node_size=700, node_color="lightblue", font_size=12)
plt.show()

"""Let's test it out:"""

# Install required dependencies (make sure they are installed first)
!pip install -U langchain-community langchain networkx matplotlib neo4j sentence-transformers transformers

# Import necessary libraries
import os
from neo4j import GraphDatabase
import networkx as nx
import matplotlib.pyplot as plt
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import Neo4jVector

# List of medical topics to load from Wikipedia
medical_queries = [
    "Cancer treatment", "Diabetes", "Heart disease", "Hypertension", "Stroke",
    "Alzheimer's disease", "Mental health", "Asthma", "Obesity", "HIV/AIDS",
    "Tuberculosis", "Kidney disease", "Liver disease", "Arthritis", "Parkinson's disease",
    "Cystic fibrosis", "Multiple sclerosis", "Epilepsy", "Chronic pain", "Infectious diseases",
    "Autoimmune diseases", "Blood disorders", "Hematology", "Dermatology", "Pediatric care",
    "Neonatal care", "Geriatrics", "Surgery", "Orthopedics", "Immunology", "Pharmacology",
    "Vaccination", "Gene therapy", "Radiology", "Pathology", "Medical imaging"
]

# Step 1: Load the Wikipedia articles related to the medical topics
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()

# Step 2: Define chunking strategy (split the document into chunks)
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents[:3])  # Limit to first 3 chunks for demo

# Step 3: Use SentenceTransformer model and wrap it with HuggingFaceEmbeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Step 4: Connect to your existing Neo4j instance
# Replace these with your Neo4j connection details if necessary
NEO4J_URI = "neo4j+s://7e04c168.databases.neo4j.io"  # Update to your Neo4j URI if different
NEO4J_USERNAME = "neo4j"  # Replace with your actual Neo4j username
NEO4J_PASSWORD = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"  # Replace with your actual Neo4j password


# Create a Neo4j driver instance using your connection details
driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))

# Step 5: Prepare the documents and their embeddings to upload to Neo4j graph
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,  # Use HuggingFace wrapper object for embeddings
    search_type="hybrid",      # Hybrid search combines traditional text and vector search
    node_label="Document",     # Label for the graph nodes (documents)
    text_node_properties=["text"],  # Store text in the 'text' property
    embedding_node_property="embedding"  # Store embeddings in this property
)

# Add documents to the Neo4j graph
vector_index.add_documents(documents)

# Step 6: Create a graph with NetworkX for visualization (optional)
G = nx.Graph()

# Add nodes based on documents (each chunk is a node in the graph)
for i, doc in enumerate(documents):
    G.add_node(i, label=doc.page_content[:100], text=doc.page_content)  # Add first 100 chars as label for visualization
    G.add_edge(i, (i+1) % len(documents), weight=1)  # Create edges between consecutive chunks for context

# Example of printing out the graph nodes and edges
print("Graph nodes:")
for node in G.nodes(data=True):
    print(node)

print("\nGraph edges:")
for edge in G.edges(data=True):
    print(edge)

# Step 7: Optional: Visualize the graph (using NetworkX or Neo4j's own visualization tools)
nx.draw(G, with_labels=True, node_size=700, node_color="lightblue", font_size=12)
plt.show()

# Step 8: Show Neo4j data (optional)
def show_neo4j_graph():
    cypher_query = "MATCH (d:Document)-[:HAS_EMBEDDING]->(e) RETURN d LIMIT 10"
    session = driver.session()
    result = session.run(cypher_query)
    for record in result:
        print(record)

show_neo4j_graph()

# Step 1: Install dependencies (if not installed)
!pip install -U langchain-community langchain networkx matplotlib neo4j sentence-transformers transformers

# Step 2: Import necessary libraries
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import Neo4jVector
import networkx as nx
import matplotlib.pyplot as plt
from transformers import pipeline

# Step 3: List of medical topics to load from Wikipedia
medical_queries = [
    "Medications", "Diabetes", "Heart disease", "Hypertension", "Medications",
    "Alzheimer's disease", "Mental health", "Asthma", "Obesity", "HIV/AIDS",
    "Tuberculosis", "Kidney disease", "Liver disease", "Arthritis", "Parkinson's disease",
    "Cystic fibrosis", "Multiple sclerosis", "Epilepsy", "Chronic pain", "Infectious diseases",
    "Autoimmune diseases", "Blood disorders", "Hematology", "Dermatology", "Pediatric care",
    "Neonatal care", "Geriatrics", "Surgery", "Orthopedics", "Immunology", "Pharmacology",
    "Vaccination", "Gene therapy", "Radiology", "Pathology", "Medical imaging"
]

# Step 4: Load the Wikipedia articles for the medical topics
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()

# Step 5: Split the documents into chunks
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents)  # Process all documents

# Step 6: Embed the documents using HuggingFace model
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")  # Wrap the model correctly

# Step 7: Store documents in Neo4j using Neo4jVector store
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,  # Use HuggingFace embeddings
    search_type="hybrid",      # Hybrid search combines traditional text and vector search
    node_label="Document",     # Label for the graph nodes (documents)
    text_node_properties=["text"],  # The properties to store in each node
    embedding_node_property="embedding"  # Store embeddings in this property
)

# Add documents to the Neo4j graph
vector_index.add_documents(documents)

# Step 8: Query the Neo4j vector store based on a user question (using vector search)
def query_graph(question):
    # Query the Neo4j graph to find the most relevant document(s) for the input question
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Step 9: Use a free question-answering model from Hugging Face (e.g., DistilBERT, BART, T5)
def generate_response_from_graph(question):
    # Retrieve the relevant document(s) from the graph
    relevant_documents = query_graph(question)

    # Join the relevant document contents to form the input for the language model
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Load Hugging Face's question-answering pipeline (use DistilBERT or BART)
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

    # Get the answer to the question from the context
    answer = qa_pipeline(question=question, context=context)

    return answer['answer']

# Step 10: Example question to ask the graph
question = "What is heart disease?"

# Step 11: Generate the response using the graph
response = generate_response_from_graph(question)
print("Response:", response)

from transformers import AutoModel

model = AutoModel.from_pretrained("microsoft/deberta-v3-large", use_auth_token=True)
from sentence_transformers import SentenceTransformer

# Load a model from sentence-transformers
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# You can then use this embedding_model to generate embeddings
hf_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Step 4: Embed the documents using HuggingFace model
hf_embeddings = HuggingFaceEmbeddings(model_name="microsoft/deberta-v3-large")  # Correct model name

# Step 5: Store documents in Neo4j using Neo4jVector store
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,  # Use HuggingFace embeddings
    search_type="hybrid",      # Hybrid search combines traditional text and vector search
    node_label="Document",     # Label for the graph nodes (documents)
    text_node_properties=["text"],  # The properties to store in each node
    embedding_node_property="embedding"  # Store embeddings in this property
)

# Add documents to the Neo4j graph
vector_index.add_documents(documents)

# Step 6: Query the Neo4j vector store based on a user question (using vector search)
def query_graph(question):
    # Query the Neo4j graph to find the most relevant document(s) for the input question
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Step 7: Use a more powerful model for question-answering from Hugging Face (DeBERTa v3 or RoBERTa)
def generate_response_from_graph(question):
    # Retrieve the relevant document(s) from the graph
    relevant_documents = query_graph(question)

    # Join the relevant document contents to form the input for the language model
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Use a more powerful question-answering model like DeBERTa or RoBERTa
    qa_pipeline = pipeline("question-answering", model="microsoft/deberta-v3-large")

    # Get the answer to the question from the context
    answer = qa_pipeline(question=question, context=context)

    return answer['answer']

# Step 8: Example question to ask the graph
question = "What is heart disease?"

# Step 9: Generate the response using the graph
response = generate_response_from_graph(question)

# Output the response
print("Response:", response)

# Step 9: Example question to ask the graph
question = "medicines used for cancer"

# Step 10: Generate the response using the graph
response = generate_response_from_graph(question)
print("Response:", response)

def generate_full_text_query(input: str) -> str:
    """
    Generate a full-text search query for a given input string.

    This function constructs a query string suitable for a full-text search.
    It processes the input string by splitting it into words and appending a
    similarity threshold (~2 changed characters) to each word, then combines
    them using the AND operator. Useful for mapping entities from user questions
    to database values, and allows for some misspelings.
    """
    full_text_query = ""
    words = [el for el in remove_lucene_chars(input).split() if el]
    for word in words[:-1]:
        full_text_query += f" {word}~2 AND"
    full_text_query += f" {words[-1]}~2"
    return full_text_query.strip()

# Fulltext index query
def structured_retriever(question: str) -> str:
    """
    Collects the neighborhood of entities mentioned
    in the question
    """
    result = ""
    entities = entity_chain.invoke({"question": question})
    for entity in entities.names:
        response = graph.query(
            """CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})
            YIELD node,score
            CALL {
              WITH node
              MATCH (node)-[r:!MENTIONS]->(neighbor)
              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output
              UNION ALL
              WITH node
              MATCH (node)<-[r:!MENTIONS]-(neighbor)
              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output
            }
            RETURN output LIMIT 50
            """,
            {"query": generate_full_text_query(entity)},
        )
        result += "\n".join([el['output'] for el in response])
    return result

print(structured_retriever("What is cancer?"))

def retriever(question: str):
    print(f"Search query: {question}")
    structured_data = structured_retriever(question)
    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]
    final_data = f"""Structured data:
{structured_data}
Unstructured data:
{"#Document ". join(unstructured_data)}
    """
    return final_data

# Condense a chat history and follow-up question into a standalone question
_template = """Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,
in its original language.
Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:"""  # noqa: E501
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)

def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:
    buffer = []
    for human, ai in chat_history:
        buffer.append(HumanMessage(content=human))
        buffer.append(AIMessage(content=ai))
    return buffer

_search_query = RunnableBranch(
    # If input includes chat_history, we condense it with the follow-up question
    (
        RunnableLambda(lambda x: bool(x.get("chat_history"))).with_config(
            run_name="HasChatHistoryCheck"
        ),  # Condense follow-up question and chat into a standalone_question
        RunnablePassthrough.assign(
            chat_history=lambda x: _format_chat_history(x["chat_history"])
        )
        | CONDENSE_QUESTION_PROMPT
        | ChatOpenAI(temperature=0)
        | StrOutputParser(),
    ),
    # Else, we have no chat history, so just pass through the question
    RunnableLambda(lambda x : x["question"]),
)

"""Next, we introduce a prompt that leverages the context provided by the integrated hybrid retriever to produce the response, completing the implementation of the RAG chain."""

template = """Answer the question based only on the following context:
{context}

Question: {question}
Use natural language and be concise.
Answer:"""
prompt = ChatPromptTemplate.from_template(template)

chain = (
    RunnableParallel(
        {
            "context": _search_query | retriever,
            "question": RunnablePassthrough(),
        }
    )
    | prompt
    | llm
    | StrOutputParser()
)

"""Finally, we can go ahead and test our hybrid RAG implementation."""

def retriever(question: str):
    print(f"Search query: {question}")

    # Retrieve unstructured data (documents that match the question)
    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]

    # Placeholder for structured data retrieval (if you have any structured data source)
    structured_data = "Structured data not implemented yet"  # You can implement this part based on your needs

    # Combine both structured and unstructured data into a final context
    final_data = f"""Structured data:
{structured_data}
Unstructured data:
{"#Document ".join(unstructured_data)}"""

    return final_data

# Step 6: Define the question-answering function
def process_query(query: str, chat_history: List[Tuple[str, str]] = None):
    # Retrieve context (documents) based on the question
    context = retriever(query)

    # Use Hugging Face Q&A model (DistilBERT) for answering
    # Assume context is joined into one large string for the Q&A model
    result = qa_pipeline(question=query, context=context)

    # Return the answer from the model
    return result['answer']

# Example usage: Process a question and get an answer
query = "What is cancer?"
chat_history = [("What causes cancer?")]

response = process_query(query, chat_history)
print(f"Answer: {response}")

"""Let's test a follow up question!"""

import os
import requests
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector
from neo4j import GraphDatabase
from transformers import pipeline

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "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"
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to insert question and answer into Neo4j
def insert_question_answer(driver, question, answer):
    query = """
    MERGE (q:Question {text: $question})
    MERGE (a:Answer {text: $answer})
    MERGE (q)-[:HAS_ANSWER]->(a)
    """
    driver.query(query, parameters={"question": question, "answer": answer})

# Use the Medical Question Answering API to get the answer
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"

    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }

    payload = {
        'query': query
    }

    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        print(f"Error: {response.status_code}")
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Integrate API response with Neo4j
def process_query_and_store(question, neo4j_driver, vector_index):
    answer = get_answer_from_api(question)
    insert_question_answer(neo4j_driver, question, answer)
    relevant_documents = query_graph(question, vector_index)
    context = " ".join([doc.page_content for doc in relevant_documents])
    return answer, context

# List of medical topics to load from Wikipedia
medical_queries = [
    "Medications", "Signs and symptoms", "Lists of diseases", ""
]

# Load Wikipedia articles
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()

# Split documents into chunks
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents)

# Embed documents using HuggingFace
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store documents in Neo4j using Neo4jVector store
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,
    search_type="hybrid",
    node_label="Document",
    text_node_properties=["text"],
    embedding_node_property="embedding",
    url=os.environ["NEO4J_URI"],  # Pass Neo4j URI here
    username=os.environ["NEO4J_USERNAME"],
    password=os.environ["NEO4J_PASSWORD"]
)

# Add documents to Neo4j
vector_index.add_documents(documents)

# Example query
question = "How do I stop hair loss?"

# Generate response using API and Neo4j
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Answer from API: {answer}")
print(f"Relevant documents from Neo4j: {context}")

import os
import requests
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector
from neo4j import GraphDatabase
from transformers import pipeline

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "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"
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to insert question and answer into Neo4j
def insert_question_answer(driver, question, answer):
    query = """
    MERGE (q:Question {text: $question})
    MERGE (a:Answer {text: $answer})
    MERGE (q)-[:HAS_ANSWER]->(a)
    """
    driver.query(query, parameters={"question": question, "answer": answer})

# Use the Medical Question Answering API to get the answer
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)
    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        print(f"Error: {response.status_code}")
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to truncate context to a given length
def truncate_context(context, max_length=512):
    # If the context length exceeds the max length, truncate it
    if len(context.split()) > max_length:
        context = " ".join(context.split()[:max_length])
    return context

# Generation with context: use T5 model to generate a response
def generate_answer(question, context):
    # Apply truncation to ensure context is within the model's token limit
    context = truncate_context(context, max_length=100)  # Adjust max_length if needed

    # Concatenate the question with the context
    input_text = f"Answer this question based on the context:\n\nQuestion: {question}\nContext: {context}\nAnswer:"

    # Use HuggingFace's pipeline for text generation (T5 model)
    generator = pipeline("text2text-generation", model="t5-small")
    generated_response = generator(input_text, max_length=150, num_return_sequences=1)

    # Return the generated answer
    return generated_response[0]['generated_text'].strip()

# Integrate API response with Neo4j and Generation
def process_query_and_store(question, neo4j_driver, vector_index):
    # Get the answer from the Medical API (this is optional)
    api_answer = get_answer_from_api(question)

    # Retrieve relevant documents from Neo4j
    relevant_documents = query_graph(question, vector_index)

    # Ensure that context is not empty and is meaningful
    if not relevant_documents:
        return "No relevant context found.", None

    # Create context from relevant documents
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Use generation to create a response based on the question and context
    generated_answer = generate_answer(question, context)

    # Store the question and generated answer in Neo4j
    insert_question_answer(neo4j_driver, question, generated_answer)

    return generated_answer, context

# List of medical topics to load from Wikipedia
medical_queries = [
    "Medications", "Signs and symptoms", "Lists of diseases"
]

# Load Wikipedia articles
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()

# Split documents into chunks
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents)

# Embed documents using HuggingFace
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store documents in Neo4j using Neo4jVector store
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,
    search_type="hybrid",
    node_label="Document",
    text_node_properties=["text"],
    embedding_node_property="embedding",
    url=os.environ["NEO4J_URI"],
    username=os.environ["NEO4J_USERNAME"],
    password=os.environ["NEO4J_PASSWORD"]
)

# Add documents to Neo4j
vector_index.add_documents(documents)

# Example query
question = "What is intravenous therapy?"

# Generate response using API and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

# Example query
question = "What is the use of paracetamol?"

# Generate response using API and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")



# Example query
question = "What is the use of paracetamol?"

# Generate response using API and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

import os
import requests
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector
from neo4j import GraphDatabase
from transformers import pipeline

# Set up environment variables
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to insert question and answer into Neo4j
def insert_question_answer(driver, question, answer):
    query = """
    MERGE (q:Question {text: $question})
    MERGE (a:Answer {text: $answer})
    MERGE (q)-[:HAS_ANSWER]->(a)
    """
    driver.query(query, parameters={"question": question, "answer": answer})

# Use the Medical Question Answering API to get the answer
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        print(f"Error: {response.status_code}")
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to truncate context to a given length
def truncate_context(context, max_length=512):
    # If the context length exceeds the max length, truncate it
    if len(context.split()) > max_length:
        context = " ".join(context.split()[:max_length])
    return context

# Integrate API response with Neo4j (using RapidAPI to generate an answer)
def process_query_and_store(question, neo4j_driver, vector_index):
    # Retrieve relevant documents from Neo4j
    relevant_documents = query_graph(question, vector_index)

    # Ensure that context is not empty and is meaningful
    if not relevant_documents:
        return "No relevant context found.", None

    # Create context from relevant documents
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Get the answer from RapidAPI using the question and context
    api_answer = get_answer_from_api(f"{question}\nContext: {context}")

    # Store the question and generated answer in Neo4j
    insert_question_answer(neo4j_driver, question, api_answer)

    return api_answer, context

medical_queries = [
    # Diseases and Conditions
    "Medications", "Signs and symptoms", "Lists of diseases", "Cancer", "Diabetes",
    "Hypertension", "Heart disease", "Obesity", "Asthma", "Infectious diseases",
    "Mental health", "Neurology", "Pain management", "Sleep disorders", "Autoimmune diseases",
    "Stroke", "Epilepsy", "Alzheimer's disease", "Parkinson's disease", "Multiple sclerosis",
    "Chronic obstructive pulmonary disease", "Kidney disease", "Liver disease", "Gastrointestinal diseases",
    "Chronic fatigue syndrome", "HIV/AIDS", "Tuberculosis", "Hepatitis", "Cystic fibrosis",
    "Endocrine disorders", "Thyroid disorders", "Hyperthyroidism", "Hypothyroidism", "Diabetes mellitus",
    "Obesity", "Anemia", "Lung cancer", "Breast cancer", "Leukemia", "Ovarian cancer", "Prostate cancer",
    "Colorectal cancer", "Skin cancer", "Pancreatic cancer", "Cervical cancer", "Esophageal cancer",

    # Treatments and Procedures
    "Surgical procedures", "Radiotherapy", "Chemotherapy", "Stem cell therapy", "Gene therapy",
    "Organ transplant", "Surgical oncology", "Cardiac surgery", "Orthopedic surgery", "Pediatric surgery",
    "Neurosurgery", "Plastic surgery", "Cosmetic surgery", "Laparoscopy", "Endoscopy", "Blood transfusion",
    "Dialysis", "Physiotherapy", "Occupational therapy", "Speech therapy", "Mental health therapy",
    "Cognitive behavioral therapy", "Electroconvulsive therapy", "Deep brain stimulation", "Acupuncture",

    # Medications and Drugs
    "Antibiotics", "Analgesics", "Antidepressants", "Antipsychotics", "Diuretics", "Antihypertensive drugs",
    "Statins", "Insulin", "Painkillers", "Anti-inflammatory drugs", "Chemotherapy drugs", "Immunosuppressants",
    "Steroids", "Vitamins and supplements", "Herbal medicine", "Vaccines", "Pain management drugs", "Opioids",

    # Health and Wellness
    "Health promotion", "Nutrition", "Physical fitness", "Mental wellness", "Preventive healthcare",
    "Dietary supplements", "Healthy eating", "Weight management", "Exercise", "Yoga", "Mindfulness",
    "Stress management", "Sleep hygiene", "Work-life balance", "Chronic illness management",

    # Mental Health
    "Mental disorders", "Anxiety disorders", "Depression", "Bipolar disorder", "Schizophrenia", "Obsessive-compulsive disorder",
    "Post-traumatic stress disorder", "Addiction", "Substance abuse", "Eating disorders", "Autism spectrum disorder",
    "Personality disorders", "Suicide prevention", "Psychotic disorders", "Mood disorders", "Psychotherapy",

    # Pediatric Care
    "Pediatric care", "Infant care", "Childhood vaccinations", "Autism spectrum disorder in children",
    "Pediatric cardiology", "Pediatric neurology", "Pediatric surgery", "Pediatric asthma", "Children's health",

    # Women's Health
    "Gynecology", "Obstetrics", "Pregnancy", "Menstrual cycle", "Menopause", "Breast cancer", "Cervical cancer",
    "Pregnancy complications", "Reproductive health", "Contraception", "Fertility treatments", "Sexual health",
    "STDs and STIs", "Pap smear", "Mammography", "Family planning", "Pregnancy tests", "Labor and delivery",
    "Postpartum care", "Hormonal imbalances", "Polycystic ovary syndrome", "Endometriosis",

    # Elderly Care and Geriatrics
    "Geriatrics", "Aging", "Dementia", "Alzheimer's disease", "Falls prevention", "Senior health",
    "Elder care", "Palliative care", "Hospice care", "Frailty in the elderly", "Osteoporosis", "Arthritis",
    "Vision loss in elderly", "Hearing loss in elderly", "Incontinence", "Geriatric nutrition", "Geriatric rehabilitation",


]


# Load Wikipedia articles
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()

# Split documents into chunks
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents)

# Embed documents using HuggingFace
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store documents in Neo4j using Neo4jVector store
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,
    search_type="hybrid",
    node_label="Document",
    text_node_properties=["text"],
    embedding_node_property="embedding",
    url=os.environ["NEO4J_URI"],
    username=os.environ["NEO4J_USERNAME"],
    password=os.environ["NEO4J_PASSWORD"]
)

# Add documents to Neo4j
vector_index.add_documents(documents)

# Example query
question = "How do I stop hair loss?"

# Generate response using RapidAPI and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

"""Support for third party widgets will remain active for the duration of the session. To disable support:"""

# Example query
question = "What is cancer?"

# Generate response using RapidAPI and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

# Example query
question = "What is cancer?"

# Generate response using RapidAPI and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

import os
import requests
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector
from neo4j import GraphDatabase
from transformers import pipeline

# Set up environment variables
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to insert question and answer into Neo4j
def insert_question_answer(driver, question, answer):
    query = """
    MERGE (q:Question {text: $question})
    MERGE (a:Answer {text: $answer})
    MERGE (q)-[:HAS_ANSWER]->(a)
    """
    driver.query(query, parameters={"question": question, "answer": answer})

# Use the Medical Question Answering API to get the answer
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        print(f"Error: {response.status_code}")
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to truncate context to a given length
def truncate_context(context, max_length=512):
    # If the context length exceeds the max length, truncate it
    if len(context.split()) > max_length:
        context = " ".join(context.split()[:max_length])
    return context

# Integrate API response with Neo4j (using RapidAPI to generate an answer)
def process_query_and_store(question, neo4j_driver, vector_index):
    # Retrieve relevant documents from Neo4j
    relevant_documents = query_graph(question, vector_index)

    # Ensure that context is not empty and is meaningful
    if not relevant_documents:
        return "No relevant context found.", None

    # Create context from relevant documents
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Get the answer from RapidAPI using the question and context
    api_answer = get_answer_from_api(f"{question}\nContext: {context}")

    # Store the question and generated answer in Neo4j
    insert_question_answer(neo4j_driver, question, api_answer)

    return api_answer, context

medical_queries = []



# Load Wikipedia articles
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()

# Split documents into chunks
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents)

# Embed documents using HuggingFace
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store documents in Neo4j using Neo4jVector store
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,
    search_type="hybrid",
    node_label="Document",
    text_node_properties=["text"],
    embedding_node_property="embedding",
    url=os.environ["NEO4J_URI"],
    username=os.environ["NEO4J_USERNAME"],
    password=os.environ["NEO4J_PASSWORD"]
)

# Add documents to Neo4j
vector_index.add_documents(documents)

# Example query
question = "What are the symptoms of skin cancer?"

# Generate response using RapidAPI and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

import os
import requests
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector
from neo4j import GraphDatabase
from transformers import pipeline

# Set up environment variables
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to insert question and answer into Neo4j
def insert_question_answer(driver, question, answer):
    query = """
    MERGE (q:Question {text: $question})
    MERGE (a:Answer {text: $answer})
    MERGE (q)-[:HAS_ANSWER]->(a)
    """
    driver.query(query, parameters={"question": question, "answer": answer})

# Use the Medical Question Answering API to get the answer
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        print(f"Error: {response.status_code}")
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to truncate context to a given length
def truncate_context(context, max_length=512):
    # If the context length exceeds the max length, truncate it
    if len(context.split()) > max_length:
        context = " ".join(context.split()[:max_length])
    return context

# Integrate API response with Neo4j (using RapidAPI to generate an answer)
def process_query_and_store(question, neo4j_driver, vector_index):
    # Retrieve relevant documents from Neo4j
    relevant_documents = query_graph(question, vector_index)

    # Ensure that context is not empty and is meaningful
    if not relevant_documents:
        return "No relevant context found.", None

    # Create context from relevant documents
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Get the answer from RapidAPI using the question and context
    api_answer = get_answer_from_api(f"{question}\nContext: {context}")

    # Store the question and generated answer in Neo4j
    insert_question_answer(neo4j_driver, question, api_answer)

    return api_answer, context

medical_queries = []



# Load Wikipedia articles
raw_documents = []
for query in medical_queries:
    print(f"Loading documents for: {query}")
    raw_documents += WikipediaLoader(query=query).load()

# Split documents into chunks
text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)
documents = text_splitter.split_documents(raw_documents)

# Embed documents using HuggingFace
hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store documents in Neo4j using Neo4jVector store
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,
    search_type="hybrid",
    node_label="Document",
    text_node_properties=["text"],
    embedding_node_property="embedding",
    url=os.environ["NEO4J_URI"],
    username=os.environ["NEO4J_USERNAME"],
    password=os.environ["NEO4J_PASSWORD"]
)

# Add documents to Neo4j
vector_index.add_documents(documents)

# Example query
question = "What are the symptoms of skin cancer?"

# Generate response using RapidAPI and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

from google.colab import output
output.enable_custom_widget_manager()

"""Support for third party widgets will remain active for the duration of the session. To disable support:"""

from google.colab import output
output.disable_custom_widget_manager()

# Example query
question = "What is ectopic?"

# Generate response using RapidAPI and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

# Example query
question = "What is ectopic?"

# Generate response using RapidAPI and Neo4j with context-based generation
neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
generated_answer, context = process_query_and_store(question, neo4j_driver, vector_index)

# Print the response
print(f"Generated Answer: {generated_answer}")
print(f"Relevant documents from Neo4j: {context}")

import os
import requests
from neo4j import GraphDatabase
import tensorflow as tf

# Set up environment variables
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Load your Gemma model (if you're using TensorFlow/Keras)
gemma_model_path = "/kaggle/input/gemma/keras/gemma_instruct_2b_en/2"
gemma_model = tf.keras.models.load_model(gemma_model_path)

# Neo4j Connection Class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Example of a simple query to check the connection
def test_neo4j_query():
    query = "MATCH (n) RETURN COUNT(n)"
    result = neo4j_driver.query(query)
    for record in result:
        print(f"Number of nodes in database: {record['COUNT(n)']}")

# Set up the Neo4j driver connection
neo4j_driver = Neo4jConnection(os.getenv("NEO4J_URI"), os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD"))

# Test connection and query
test_neo4j_query()

import os
import requests
from neo4j import GraphDatabase
from langchain.vectorstores import Neo4jVector
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import pipeline

# Set up environment variables for Neo4j
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Neo4j connection class to interact with the database
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to insert question and answer into Neo4j
def insert_question_answer(driver, question, answer, conversation_id):
    query = """
    MERGE (q:Question {text: $question})
    MERGE (a:Answer {text: $answer})
    MERGE (c:Conversation {id: $conversation_id})
    MERGE (q)-[:HAS_ANSWER]->(a)
    MERGE (c)-[:HAS_QUESTION]->(q)
    MERGE (c)-[:HAS_ANSWER]->(a)
    """
    driver.query(query, parameters={"question": question, "answer": answer, "conversation_id": conversation_id})

# Function to get the answer from an API (e.g., medical question answering API)
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "YOUR_API_KEY",  # Replace with your actual API key
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)
    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        print(f"Error: {response.status_code}")
        return "Error in fetching the answer."

# Function to perform similarity search using Neo4j embeddings
def query_graph(question, vector_index):
    # Perform similarity search on the stored embeddings in Neo4j
    results = vector_index.similarity_search(query=question, k=3)  # Retrieve top-3 most relevant documents
    return results

# Function to process the query and generate a response
def process_query_and_store(question, neo4j_driver, vector_index, conversation_id):
    relevant_documents = query_graph(question, vector_index)  # Retrieve relevant documents based on the embeddings
    if not relevant_documents:
        return "No relevant context found.", None

    # Create context from the relevant documents
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Use the context to get an answer (from the API or another model)
    api_answer = get_answer_from_api(f"{question}\nContext: {context}")

    # Store the question, generated answer, and conversation ID in Neo4j
    insert_question_answer(neo4j_driver, question, api_answer, conversation_id)

    return api_answer, context

# Start the conversation loop
def start_conversation():
    neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])

    conversation_id = str(os.urandom(16).hex())  # Create a unique conversation ID (you could use a timestamp)

    print("Welcome to the QA Chatbot! Type 'exit' to end the conversation.")

    while True:
        # Get user input
        user_question = input("You: ")
        if user_question.lower() == "exit":
            print("Ending conversation.")
            break

        # Process the question, get the answer, and store the history
        generated_answer, context = process_query_and_store(user_question, neo4j_driver, vector_index, conversation_id)

        # Show the generated answer
        print(f"Chatbot: {generated_answer}")

    neo4j_driver.close()

# Load existing Neo4j embeddings and use them for similarity search
# Assuming embeddings are already stored in Neo4j and vector_index is available for querying

# Initialize Neo4jVector with correct parameters
vector_index = Neo4jVector.from_existing_graph(
    embedding=hf_embeddings,
    search_type="hybrid",
    node_label="Document",
    text_node_properties=["text"],
    embedding_node_property="embedding",
    url=os.environ["NEO4J_URI"],
    username=os.environ["NEO4J_USERNAME"],
    password=os.environ["NEO4J_PASSWORD"]
)


# Start a conversation
start_conversation()

import os
import requests
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector
from neo4j import GraphDatabase
from transformers import pipeline

# Set up environment variables
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to insert question and answer into Neo4j
def insert_question_answer(driver, question, answer):
    query = """
    MERGE (q:Question {text: $question})
    MERGE (a:Answer {text: $answer})
    MERGE (q)-[:HAS_ANSWER]->(a)
    """
    driver.query(query, parameters={"question": question, "answer": answer})

# Function to call RapidAPI for medical question answering
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",  # Replace with your API key
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        answer = response.json().get('answer', 'No answer found.')
        print(f"API Response: {answer}")  # Debugging line
        return answer
    else:
        print(f"Error: {response.status_code}")
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    # Perform similarity search on the stored embeddings in Neo4j
    print(f"Querying Neo4j for: {question}")  # Debugging line
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to truncate context to a given length
def truncate_context(context, max_length=512):
    # If the context length exceeds the max length, truncate it
    if len(context.split()) > max_length:
        context = " ".join(context.split()[:max_length])
    return context

# Generate the response based on the question
def generate_response_from_graph(question, neo4j_driver, vector_index):
    # First, check if we can get an answer from the RapidAPI
    api_answer = get_answer_from_api(question)

    # If we get a valid answer from API, return that answer
    if api_answer and api_answer != 'No answer found.' and api_answer != "Error in fetching the answer.":
        return api_answer

    # If no answer from API, retrieve relevant documents from Neo4j
    relevant_documents = query_graph(question, vector_index)

    # If no relevant documents found, return a fallback answer
    if not relevant_documents:
        print("No relevant documents found in Neo4j.")  # Debugging line
        return "No relevant context found."

    # Create context from relevant documents
    context = " ".join([doc.page_content for doc in relevant_documents])

    # Truncate context to ensure it's not too long for the Hugging Face model
    context = truncate_context(context)

    # Load Hugging Face's question-answering pipeline (use DistilBERT)
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

    # Get the answer to the question from the context
    answer = qa_pipeline(question=question, context=context)

    print(f"Generated Answer: {answer['answer']}")  # Debugging line
    return answer['answer']

# Chatbot loop to interact with the user
def chatbot_loop():
    # Initialize the Neo4j connection
    neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])

    # Load Neo4j embeddings (assuming embeddings are already stored in Neo4j)
    vector_index = Neo4jVector.from_existing_graph(
        embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"),
        search_type="hybrid",
        node_label="Document",
        text_node_properties=["text"],
        embedding_node_property="embedding",
        url=os.environ["NEO4J_URI"],
        username=os.environ["NEO4J_USERNAME"],
        password=os.environ["NEO4J_PASSWORD"]
    )

    print("Welcome to the medical QA chatbot! Type 'exit' to end the conversation.")

    while True:
        # Get user input
        user_question = input("You: ")

        # If the user types 'exit', end the conversation
        if user_question.lower() == "exit":
            print("Ending conversation.")
            break

        # Generate the answer based on the question
        answer = generate_response_from_graph(user_question, neo4j_driver, vector_index)

        # Print the generated answer
        print(f"Chatbot: {answer}")

    neo4j_driver.close()

# Run the chatbot
chatbot_loop()

import os
import requests
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector
from neo4j import GraphDatabase
from transformers import pipeline
import gradio as gr

# Set up environment variables (you can skip this if you're using them in your environment)
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to call RapidAPI for medical question answering
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",  # Replace with your API key
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        answer = response.json().get('answer', 'No answer found.')
        return answer
    else:
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to format the context from Neo4j
def format_neo4j_context(documents):
    context = ""
    for doc in documents:
        context += f"Text: {doc.page_content}\n\n"
    return context

# Generate the response based on the question
def generate_response_from_graph(question, neo4j_driver, vector_index):
    # First, check if we can get an answer from the RapidAPI
    api_answer = get_answer_from_api(question)

    if api_answer != "Error in fetching the answer." and api_answer != "No answer found.":
        return api_answer  # Return API answer if valid

    # If no valid answer from API, use Neo4j documents
    relevant_documents = query_graph(question, vector_index)

    if not relevant_documents:
        return "No relevant context found."

    # Format context from the relevant documents retrieved from Neo4j
    context = format_neo4j_context(relevant_documents)

    # Use Hugging Face's pipeline to extract a more structured answer from the context
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
    answer = qa_pipeline(question=question, context=context)

    return f"Answer from API: {api_answer}\n\nContext from Neo4j:\n{context}\n\nExtracted Answer from Context: {answer['answer']}"

# Gradio UI
def chatbot_interface(user_input):
    # Initialize the Neo4j connection and vector store (embedding)
    neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
    vector_index = Neo4jVector.from_existing_graph(
        embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"),
        search_type="hybrid",
        node_label="Document",
        text_node_properties=["text"],
        embedding_node_property="embedding",
        url=os.environ["NEO4J_URI"],
        username=os.environ["NEO4J_USERNAME"],
        password=os.environ["NEO4J_PASSWORD"]
    )

    # Generate response from API and Neo4j
    response = generate_response_from_graph(user_input, neo4j_driver, vector_index)

    return response

# Create Gradio interface
iface = gr.Interface(fn=chatbot_interface, inputs="text", outputs="text", live=True)

# Launch the Gradio interface
iface.launch()

import os
import requests
import gradio as gr
import joblib
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from PIL import Image
import numpy as np
import pandas as pd
from keras.models import load_model
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import LabelEncoder
from transformers import pipeline
from neo4j import GraphDatabase
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector

# Set up environment variables
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Load models and vectorizers
voting_classifier = joblib.load('/content/voting_classifier_model_Disease_pred_97_percent_acc-2.pkl')
tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer_disease_nlp.joblib')
label_encoder = joblib.load('/content/label_encoder_disease_nlp.joblib')
cnn_model = load_model('/content/resnet.h5')

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to call RapidAPI for medical question answering
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",  # Replace with your API key
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to format the context from Neo4j
def format_neo4j_context(documents):
    context = ""
    for doc in documents:
        context += f"Text: {doc.page_content}\n\n"
    return context

# Generate the response based on the question
def generate_response_from_graph(question, neo4j_driver, vector_index):
    # First, check if we can get an answer from the RapidAPI
    api_answer = get_answer_from_api(question)

    if api_answer != "Error in fetching the answer." and api_answer != "No answer found.":
        return api_answer  # Return API answer if valid

    # If no valid answer from API, use Neo4j documents
    relevant_documents = query_graph(question, vector_index)

    if not relevant_documents:
        return "No relevant context found."

    # Format context from the relevant documents retrieved from Neo4j
    context = format_neo4j_context(relevant_documents)

    # Use Hugging Face's pipeline to extract a more structured answer from the context
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
    answer = qa_pipeline(question=question, context=context)

    return f"Answer from API: {api_answer}\n\nContext from Neo4j:\n{context}\n\nExtracted Answer from Context: {answer['answer']}"

# Transform symptoms to features using TF-IDF vectorizer
def transform_input_to_features(symptoms):
    # Transform the input symptoms (text) using the TF-IDF vectorizer
    transformed_input = tfidf_vectorizer.transform([symptoms])
    return transformed_input

# Predict disease based on the symptoms
def predict_disease(symptoms):
    # Convert input symptoms to features
    features = transform_input_to_features(symptoms)

    # Use the Voting Classifier to predict the disease
    predicted_label = voting_classifier.predict(features)

    # Decode the predicted label to get the disease name
    disease = label_encoder.inverse_transform(predicted_label)

    return disease[0]  # Return the disease name as a string

# Function to process skin disease image using CNN
def process_skin_disease_image(image):
    # Preprocess the image (assuming you're using CNN with preprocessing)
    img = np.array(image.resize((224, 224)))  # Resize to expected input size for the model (224x224 for ResNet)
    img = img / 255.0  # Normalize if needed, or apply other preprocessing steps
    img = np.expand_dims(img, axis=0)  # Add batch dimension

    # Predict the skin disease using the CNN model
    disease_class = cnn_model.predict(img)

    # Map the predicted class to the corresponding disease name
    class_index = np.argmax(disease_class, axis=1)
    disease_name = label_encoder.inverse_transform(class_index)

    return disease_name[0]

# Gradio Interface
def chatbot_interface(user_input, history=[], image=None):
    # Step 1: Handle Skin Disease Prediction from Image (if an image is provided)
    if image:
        skin_disease = process_skin_disease_image(image)
        history.append(("Image Input: Skin Disease", f"Predicted Skin Disease: {skin_disease}"))

    # Step 2: Handle Disease Prediction from Symptoms (Text)
    disease_prediction = predict_disease(user_input)  # Get disease prediction based on symptoms

    # Step 3: Generate response from API and Neo4j
    neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
    vector_index = Neo4jVector.from_existing_graph(
        embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"),
        search_type="hybrid",
        node_label="Document",
        text_node_properties=["text"],
        embedding_node_property="embedding",
        url=os.environ["NEO4J_URI"],
        username=os.environ["NEO4J_USERNAME"],
        password=os.environ["NEO4J_PASSWORD"]
    )

    # Generate response from Neo4j (or API if necessary)
    response = generate_response_from_graph(user_input, neo4j_driver, vector_index)

    # Append the disease prediction to the conversation history
    history.append(("User Input: " + user_input, "Predicted Disease: " + disease_prediction))
    history.append(("Bot Response:", response))

    # Return both the conversation history and current state (for Gradio to keep track of conversation)
    return history, history

# Create Gradio interface without 'live=True'
iface = gr.Interface(
    fn=chatbot_interface,
    inputs=["text", "state", gr.Image(type="pil")],  # Updated for Gradio v3.x
    outputs=["chatbot", "state"],
    theme="compact",  # Using "compact" theme instead of "huggingface"
    title="Medical Chatbot",  # Title for the Gradio interface
    description="A medical assistant chatbot to help with your queries and symptom analysis.",  # Description
    allow_flagging="never",  # Disable flagging of inappropriate content
    analytics_enabled=False,  # Disable analytics
)

# Launch the Gradio interface
iface.launch()

loaded_model = joblib.load('/content/voting_classifier_model_Disease_pred_97_percent_acc-2.pkl')

!pip install gradio
import os
import requests
import gradio as gr
import joblib
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from PIL import Image
import numpy as np
import pandas as pd
from keras.models import load_model
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import LabelEncoder
from transformers import pipeline
from neo4j import GraphDatabase
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector

# Set up environment variables
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Load models and vectorizers
voting_classifier = joblib.load('/content/voting_classifier_model_Disease_pred_97_percent_acc-2.pkl')
tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer_disease_nlp.joblib')
label_encoder = joblib.load('/content/label_encoder_disease_nlp.joblib')
cnn_model = load_model('/content/resnet.h5')

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to call RapidAPI for medical question answering
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",  # Replace with your API key
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to format the context from Neo4j
def format_neo4j_context(documents):
    context = ""
    for doc in documents:
        context += f"Text: {doc.page_content}\n\n"
    return context

# Generate the response based on the question
def generate_response_from_graph(question, neo4j_driver, vector_index):
    # First, check if we can get an answer from the RapidAPI
    api_answer = get_answer_from_api(question)

    if api_answer != "Error in fetching the answer." and api_answer != "No answer found.":
        return api_answer  # Return API answer if valid

    # If no valid answer from API, use Neo4j documents
    relevant_documents = query_graph(question, vector_index)

    if not relevant_documents:
        return "No relevant context found."

    # Format context from the relevant documents retrieved from Neo4j
    context = format_neo4j_context(relevant_documents)

    # Use Hugging Face's pipeline to extract a more structured answer from the context
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
    answer = qa_pipeline(question=question, context=context)

    return f"Answer from API: {api_answer}\n\nContext from Neo4j:\n{context}\n\nExtracted Answer from Context: {answer['answer']}"

# Transform symptoms to features using TF-IDF vectorizer
def transform_input_to_features(symptoms):
    # Transform the input symptoms (text) using the TF-IDF vectorizer
    transformed_input = tfidf_vectorizer.transform([symptoms])
    return transformed_input

# Predict disease based on the symptoms
def predict_disease(symptoms):
    # Convert input symptoms to features
    features = transform_input_to_features(symptoms)

    # Use the Voting Classifier to predict the disease
    predicted_label = voting_classifier.predict(features)

    # Decode the predicted label to get the disease name
    disease = label_encoder.inverse_transform(predicted_label)

    return disease[0]  # Return the disease name as a string

# Function to process skin disease image using CNN
def process_skin_disease_image(image):
    # Preprocess the image (assuming you're using CNN with preprocessing)
    img = np.array(image.resize((224, 224)))  # Resize to expected input size for the model (224x224 for ResNet)
    img = img / 255.0  # Normalize if needed, or apply other preprocessing steps
    img = np.expand_dims(img, axis=0)  # Add batch dimension

    # Predict the skin disease using the CNN model
    disease_class = cnn_model.predict(img)

    # Map the predicted class to the corresponding disease name
    class_index = np.argmax(disease_class, axis=1)
    disease_name = label_encoder.inverse_transform(class_index)

    return disease_name[0]

# Gradio Interface
def chatbot_interface(user_input, history=[], image=None):
    # Step 1: Handle Skin Disease Prediction from Image (if an image is provided)
    if image:
        skin_disease = process_skin_disease_image(image)
        history.append(("Image Input: Skin Disease", f"Predicted Skin Disease: {skin_disease}"))

    # Step 2: Handle Disease Prediction from Symptoms (Text)
    disease_prediction = predict_disease(user_input)  # Get disease prediction based on symptoms

    # Step 3: Generate response from API and Neo4j
    neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
    vector_index = Neo4jVector.from_existing_graph(
        embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"),
        search_type="hybrid",
        node_label="Document",
        text_node_properties=["text"],
        embedding_node_property="embedding",
        url=os.environ["NEO4J_URI"],
        username=os.environ["NEO4J_USERNAME"],
        password=os.environ["NEO4J_PASSWORD"]
    )

    # Generate response from Neo4j (or API if necessary)
    response = generate_response_from_graph(user_input, neo4j_driver, vector_index)

    # Append the disease prediction to the conversation history
    history.append(("User Input: " + user_input, "Predicted Disease: " + disease_prediction))
    history.append(("Bot Response:", response))

    # Return both the conversation history and current state (for Gradio to keep track of conversation)
    return history, history

# Create Gradio interface without 'live=True'
iface = gr.Interface(
    fn=chatbot_interface,
    inputs=["text", "state", gr.Image(type="pil")],  # Updated for Gradio v3.x
    outputs=["chatbot", "state"],
    theme="compact",  # Using "compact" theme instead of "huggingface"
    title="Medical Chatbot",  # Title for the Gradio interface
    description="A medical assistant chatbot to help with your queries and symptom analysis.",  # Description
    allow_flagging="never",  # Disable flagging of inappropriate content
    analytics_enabled=False,  # Disable analytics
)

# Launch the Gradio interface
iface.launch()

!pip install gradio
import os
import requests
import gradio as gr
import joblib
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
from PIL import Image
import numpy as np
import pandas as pd
from keras.models import load_model
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import LabelEncoder
from transformers import pipeline
from neo4j import GraphDatabase
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Neo4jVector

# Set up environment variables
os.environ["NEO4J_URI"] = "neo4j+s://7e04c168.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "MZeyMbDhpcRbRKo0giTM60N8T64PAF6qJUhkiPtnHuU"

# Load models and vectorizers
voting_classifier = joblib.load('/content/voting_classifier_model_Disease_pred_97_percent_acc-2.pkl')
tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer_disease_nlp.joblib')
label_encoder = joblib.load('/content/label_encoder_disease_nlp.joblib')
cnn_model = load_model('/content/cnn_model.h5')

# Define the Neo4j connection class
class Neo4jConnection:
    def __init__(self, uri, user, pwd):
        self.uri = uri
        self.user = user
        self.pwd = pwd
        self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.pwd))

    def close(self):
        self.driver.close()

    def query(self, query, parameters=None):
        session = self.driver.session()
        result = session.run(query, parameters)
        session.close()
        return result

# Function to call RapidAPI for medical question answering
def get_answer_from_api(query):
    url = "https://medical-question-answering.p.rapidapi.com/Project/use_gmas_api/"
    headers = {
        "x-rapidapi-key": "0ebd7a019amsh918697563e33c1ap1e5cb9jsn93f4fda8cfc4",  # Replace with your API key
        "x-rapidapi-host": "medical-question-answering.p.rapidapi.com"
    }
    payload = {'query': query}
    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        return response.json().get('answer', 'No answer found.')
    else:
        return "Error in fetching the answer."

# Query Neo4j graph based on a user question
def query_graph(question, vector_index):
    results = vector_index.similarity_search(query=question, k=1)  # Retrieve top-k relevant documents
    return results

# Function to format the context from Neo4j
def format_neo4j_context(documents):
    context = ""
    for doc in documents:
        context += f"Text: {doc.page_content}\n\n"
    return context

# Generate the response based on the question
def generate_response_from_graph(question, neo4j_driver, vector_index):
    # First, check if we can get an answer from the RapidAPI
    api_answer = get_answer_from_api(question)

    if api_answer != "Error in fetching the answer." and api_answer != "No answer found.":
        return api_answer  # Return API answer if valid

    # If no valid answer from API, use Neo4j documents
    relevant_documents = query_graph(question, vector_index)

    if not relevant_documents:
        return "No relevant context found."

    # Format context from the relevant documents retrieved from Neo4j
    context = format_neo4j_context(relevant_documents)

    # Use Hugging Face's pipeline to extract a more structured answer from the context
    qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")
    answer = qa_pipeline(question=question, context=context)

    return f"Answer from API: {api_answer}\n\nContext from Neo4j:\n{context}\n\nExtracted Answer from Context: {answer['answer']}"

# Transform symptoms to features using TF-IDF vectorizer
def transform_input_to_features(symptoms):
    # Transform the input symptoms (text) using the TF-IDF vectorizer
    transformed_input = tfidf_vectorizer.transform([symptoms])
    return transformed_input

# Predict disease based on the symptoms
def predict_disease(symptoms):
    # Convert input symptoms to features
    features = transform_input_to_features(symptoms)

    # Use the Voting Classifier to predict the disease
    predicted_label = voting_classifier.predict(features)

    # Decode the predicted label to get the disease name
    disease = label_encoder.inverse_transform(predicted_label)

    return disease[0]  # Return the disease name as a string

# Function to process skin disease image using CNN
def process_skin_disease_image(image):
    # Preprocess the image (assuming you're using CNN with preprocessing)
    img = np.array(image.resize((224, 224)))  # Resize to expected input size for the model (224x224 for ResNet)
    img = img / 255.0  # Normalize if needed, or apply other preprocessing steps
    img = np.expand_dims(img, axis=0)  # Add batch dimension

    # Predict the skin disease using the CNN model
    disease_class = cnn_model.predict(img)

    # Map the predicted class to the corresponding disease name
    class_index = np.argmax(disease_class, axis=1)
    disease_name = label_encoder.inverse_transform(class_index)

    return disease_name[0]

# Gradio Interface
def chatbot_interface(user_input, history=[], image=None):
    # Step 1: Handle Skin Disease Prediction from Image (if an image is provided)
    if image:
        skin_disease = process_skin_disease_image(image)
        history.append(("Image Input: Skin Disease", f"Predicted Skin Disease: {skin_disease}"))

    # Step 2: Handle Disease Prediction from Symptoms (Text)
    disease_prediction = predict_disease(user_input)  # Get disease prediction based on symptoms

    # Step 3: Generate response from API and Neo4j
    neo4j_driver = Neo4jConnection(os.environ["NEO4J_URI"], os.environ["NEO4J_USERNAME"], os.environ["NEO4J_PASSWORD"])
    vector_index = Neo4jVector.from_existing_graph(
        embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"),
        search_type="hybrid",
        node_label="Document",
        text_node_properties=["text"],
        embedding_node_property="embedding",
        url=os.environ["NEO4J_URI"],
        username=os.environ["NEO4J_USERNAME"],
        password=os.environ["NEO4J_PASSWORD"]
    )

    # Generate response from Neo4j (or API if necessary)
    response = generate_response_from_graph(user_input, neo4j_driver, vector_index)

    # Append the disease prediction to the conversation history
    history.append(("User Input: " + user_input, "Predicted Disease: " + disease_prediction))
    history.append(("Bot Response:", response))

    # Return both the conversation history and current state (for Gradio to keep track of conversation)
    return history, history

# Create Gradio interface without 'live=True'
iface = gr.Interface(
    fn=chatbot_interface,
    inputs=["text", "state", gr.Image(type="pil")],  # Updated for Gradio v3.x
    outputs=["chatbot", "state"],
    theme="compact",  # Using "compact" theme instead of "huggingface"
    title="Medical Chatbot",  # Title for the Gradio interface
    description="A medical assistant chatbot to help with your queries and symptom analysis.",  # Description
    allow_flagging="never",  # Disable flagging of inappropriate content
    analytics_enabled=False,  # Disable analytics
)

# Launch the Gradio interface
iface.launch()

